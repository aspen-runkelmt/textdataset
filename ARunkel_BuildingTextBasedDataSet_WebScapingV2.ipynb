{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Text-Based Data Set by Web Scraping\n",
    "This notebook creates a text data set stored in a database. (WAS UNSUCCESSFUL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests #requests package to get the pages\n",
    "\n",
    "from bs4 import BeautifulSoup #beautiful soup to process/parse the pages\n",
    "from bs4.element import Comment\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Builds a database with the following fields\n",
    "\n",
    "* `dt`: the date and time when we pulled the page.\n",
    "* `url`: the specific URL we pulled from contained in anasoundcloud_websites.txt. Ex. https://soundcloud.com/anewangle/sea-change-8-nichole-heyer.\n",
    "* `text`: the text of the `url`. \n",
    "* `pulled`: A boolean that is TRUE if we've tried to pull the text from the URL. This is useful for keeping track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(\"anasoundcloud_database.db\") #creates database\n",
    "cur = db.cursor() #cursor allows us to talk to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates table in database\n",
    "cur.execute('''DROP TABLE IF EXISTS site_text''')\n",
    "cur.execute('''CREATE TABLE site_text ( \n",
    "    dt DATETIME,\n",
    "    url TEXT,\n",
    "    text TEXT,\n",
    "    pulled BOOLEAN)''')\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"anasoundcloud_dataset_clean.txt\"\n",
    "\n",
    "#fills table with data\n",
    "with open(input_file,'r', encoding=\"Latin-1\") as ifile:\n",
    "    for idx, line in enumerate(ifile.readlines()):\n",
    "        line = line.strip().split(\",\")\n",
    "        cur.execute('''\n",
    "            INSERT INTO site_text (url, text, pulled)\n",
    "            VALUES(?,?,?)''', line)\n",
    "        \n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reads in list of websites from anasoundcloud_websites.txt file\n",
    "Tested my code using the \"anasoundcould_test.txt\" file which only contains 1 url before running it on all of the urls. \"anasoundcould_websites.txt\" contains all urls for all episodes through November 26, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads in list of A New Angle Soundcloud links as `sites`\n",
    "sites = []\n",
    "\n",
    "with open(\"anasoundcloud_test.txt\",'r') as infile :\n",
    "#with open(\"anasoundcloud_websites.txt\",'r') as infile :\n",
    "    for line in infile :\n",
    "        sites.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prints all links in `sites`\n",
    "print(sites)\n",
    "r = requests.get(sites[0])\n",
    "#checks HTTP response status codes -> 200 = good\n",
    "#r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracts visible text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]', 'div']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_text = ''\n",
    "\n",
    "for url in sites :\n",
    "    try :\n",
    "        r = requests.get(site)\n",
    "        if r.status_code == 200 :\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            texts = soup.findAll(text=True)\n",
    "            visible_texts = filter(tag_visible, texts) \n",
    "            page_text = \" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "        sql_query = ('INSERT INTO site_text(base_url, url, text, pulled)'\n",
    "                     'VALUES(?,?,?,?)')\n",
    "        cur.execute(sql_query,[sites[0],site, page_text,'1'])\n",
    "\n",
    "        print(\"Completed pull for {} .\".format(site))\n",
    "\n",
    "        db.commit()\n",
    "    except :\n",
    "        pass ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in sites :\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    links = [url] # make sure the main page is in there. \n",
    "    \n",
    "    # Now we have all the links. Let's just load them into the DB. We'll do the \n",
    "    # page pulls and parsing separately. \n",
    "    \n",
    "    for link in set(links) : # wrap a `set` around it so that we don't get duplicates\n",
    "        new_row = [datetime.datetime.now(),\n",
    "                   url,\n",
    "                   link,\n",
    "                   \"\", # empty string for text\n",
    "                   False]\n",
    "        \n",
    "        cur.execute('''INSERT INTO site_text (dt,url,text,pulled) \n",
    "               VALUES (?,?,?,?)''',new_row)\n",
    "        \n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract text and stores it in a dictionary that has the url as the key and the value is the text.\n",
    "#from John via stackoverflow\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]', 'div']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f1988e3f2ad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mvisible_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_visible\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0manasc_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvisible_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "anasc_text = ''\n",
    "\n",
    "for link in sites :\n",
    "    try :\n",
    "        r = requests.get(link)\n",
    "    except :\n",
    "        pass \n",
    "    \n",
    "    if r.status_code == 200 :\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        texts = soup.findAll(text=True)\n",
    "        visible_texts = filter(tag_visible, texts) \n",
    "        anasc_text[link] = \" \".join(t.strip() for t in visible_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
